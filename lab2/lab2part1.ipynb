{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('popular')\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"text3.txt\", \"r\")\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a few weeks before my own marriage, during the days when I was\n",
      "still sharing rooms with Holmes in Baker Street, that he came home from\n",
      "an afternoon stroll to find a letter on the table waiting for him.\n",
      "I had remained in-doors all day, for the weather had taken a sudden\n",
      "turn to rain, with high autumnal winds, and the jezail bullet which I\n",
      "had brought back in one of my limbs as a relic of my Afghan campaign,\n",
      "throbbed with dull persistency. With my body in one easy-chair and my\n",
      "legs upon another, I had surrounded myself with a cloud of newspapers,\n",
      "until at last, saturated with the news of the day, I tossed them all\n",
      "aside and lay listless, watching the huge crest and monogram upon the\n",
      "envelope upon the table, and wondering lazily who my friendвЂ™s noble\n",
      "correspondent could be.\n",
      "\n",
      "\"Here is a very fashionable epistle,\" I remarked, as he entered. \"Your\n",
      "morning letters, if I remember right, were from a fish-monger and a\n",
      "tide-waiter.\"\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a few weeks before my own marriage, during the days when I was\\nstill sharing rooms with Holmes in Baker Street, that he came home from\\nan afternoon stroll to find a letter on the table waiting for him.', 'I had remained in-doors all day, for the weather had taken a sudden\\nturn to rain, with high autumnal winds, and the jezail bullet which I\\nhad brought back in one of my limbs as a relic of my Afghan campaign,\\nthrobbed with dull persistency.', 'With my body in one easy-chair and my\\nlegs upon another, I had surrounded myself with a cloud of newspapers,\\nuntil at last, saturated with the news of the day, I tossed them all\\naside and lay listless, watching the huge crest and monogram upon the\\nenvelope upon the table, and wondering lazily who my friendвЂ™s noble\\ncorrespondent could be.', '\"Here is a very fashionable epistle,\" I remarked, as he entered.', '\"Your\\nmorning letters, if I remember right, were from a fish-monger and a\\ntide-waiter.\"']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sents = sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['With', 'my', 'body', 'in', 'one', 'easy-chair', 'and', 'my', 'legs', 'upon', 'another', ',', 'I', 'had', 'surrounded', 'myself', 'with', 'a', 'cloud', 'of', 'newspapers', ',', 'until', 'at', 'last', ',', 'saturated', 'with', 'the', 'news', 'of', 'the', 'day', ',', 'I', 'tossed', 'them', 'all', 'aside', 'and', 'lay', 'listless', ',', 'watching', 'the', 'huge', 'crest', 'and', 'monogram', 'upon', 'the', 'envelope', 'upon', 'the', 'table', ',', 'and', 'wondering', 'lazily', 'who', 'my', 'friendвЂ™s', 'noble', 'correspondent', 'could', 'be', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "words = word_tokenize(sents[2])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['With', 'my', 'body', 'in', 'one', 'easy', 'chair', 'and', 'my', 'legs', 'upon', 'another', 'I', 'had', 'surrounded', 'myself', 'with', 'a', 'cloud', 'of', 'newspapers', 'until', 'at', 'last', 'saturated', 'with', 'the', 'news', 'of', 'the', 'day', 'I', 'tossed', 'them', 'all', 'aside', 'and', 'lay', 'listless', 'watching', 'the', 'huge', 'crest', 'and', 'monogram', 'upon', 'the', 'envelope', 'upon', 'the', 'table', 'and', 'wondering', 'lazily', 'who', 'my', 'friendвЂ', 's', 'noble', 'correspondent', 'could', 'be']\n",
      "\n",
      "num of words: 62\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(sents[2])\n",
    "print(words)\n",
    "print(\"\\nnum of words: \" + len(words).__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weeks', 'marriage', 'days', 'still', 'sharing', 'rooms', 'Holmes', 'Baker', 'Street', 'came', 'home', 'afternoon', 'stroll', 'find', 'letter', 'table', 'waiting', 'remained', 'doors', 'day', 'weather', 'taken', 'sudden', 'turn', 'rain', 'high', 'autumnal', 'winds', 'jezail', 'bullet', 'brought', 'back', 'one', 'limbs', 'relic', 'Afghan', 'campaign', 'throbbed', 'dull', 'persistency', 'body', 'one', 'easy', 'chair', 'legs', 'upon', 'another', 'surrounded', 'cloud', 'newspapers', 'last', 'saturated', 'news', 'day', 'tossed', 'aside', 'lay', 'listless', 'watching', 'huge', 'crest', 'monogram', 'upon', 'envelope', 'upon', 'table', 'wondering', 'lazily', 'friendвЂ', 'noble', 'correspondent', 'could', 'fashionable', 'epistle', 'remarked', 'entered', 'morning', 'letters', 'remember', 'right', 'fish', 'monger', 'tide', 'waiter']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "clean_words = [w for w in words if not w.lower() in stop_words]\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 9), ('a', 8), ('I', 7), ('my', 6), ('and', 6), ('with', 5), ('in', 4), ('had', 4), ('of', 4), ('upon', 3)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>I</td>\n",
       "      <td>my</td>\n",
       "      <td>and</td>\n",
       "      <td>with</td>\n",
       "      <td>in</td>\n",
       "      <td>had</td>\n",
       "      <td>of</td>\n",
       "      <td>upon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1  2   3    4     5   6    7   8     9\n",
       "word       the  a  I  my  and  with  in  had  of  upon\n",
       "frequency    9  8  7   6    6     5   4    4   4     3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(text)\n",
    "\n",
    "freq = nltk.FreqDist(words)\n",
    "freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "print(freq[:10])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(freq[:10], columns=['word', 'frequency']).T\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c348296249ce99cf8d1aba66a164ec6bcd6070e53e1052dbd1a961be0e87d504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
